# -*- coding: utf-8 -*-
"""tf-rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdoEHIToZorNVS-hmn3yiA6pTCMvcpfn

This is a TensorFlow RNN implementation
"""

import numpy as np
import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""data I/O"""

data = open('input.txt', 'r').read() # should be simple plain text file
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print ('data has %d characters, %d unique.' % (data_size, vocab_size))
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }

"""hyperparameters"""

hidden_size = 100 # size of hidden layer of neurons
seq_length = 25 # number of steps to unroll the RNN for
learning_rate = 1e-1

"""model parameters"""

Wxh = tf.Variable(tf.random.normal([hidden_size, vocab_size])*0.01) # input to hidden
Whh = tf.Variable(tf.random.normal([hidden_size, hidden_size])*0.01) # hidden to hidden
Why = tf.Variable(tf.random.normal([vocab_size, hidden_size])*0.01) # hidden to output
bh = tf.Variable(tf.zeros([hidden_size, 1])) # hidden bias
by = tf.Variable(tf.zeros([vocab_size, 1])) # output bias

"""function

inputs,targets are both list of integers.
hprev is Hx1 array of initial hidden state
returns the loss, gradients on model parameters, and last hidden state
"""

def lossFun(inputs, targets, hprev):
  xs, hs, ys, ps = {}, {}, {}, {}
  hs[-1] = tf.convert_to_tensor(hprev.numpy()) # convert numpy array to tensor
  loss = 0
  # forward pass
  for t in range(len(inputs)):
    xs[t] = tf.zeros([vocab_size,1]) # encode in 1-of-k representation
    xs[t] = tf.tensor_scatter_nd_update(xs[t], [[inputs[t], 0]], [1])
    hs[t] = tf.tanh(tf.matmul(Wxh, xs[t]) + tf.matmul(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = tf.matmul(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = tf.exp(ys[t]) / tf.reduce_sum(tf.exp(ys[t])) # probabilities for next chars
    loss += -tf.math.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
  # backward pass: compute gradients going backwards
  dWxh, dWhh, dWhy = tf.zeros_like(Wxh), tf.zeros_like(Whh), tf.zeros_like(Why)
  dbh, dby = tf.zeros_like(bh), tf.zeros_like(by)
  dhnext = tf.zeros_like(hs[0])
  for t in reversed(range(len(inputs))):
    dy = tf.identity(ps[t])
    dy = tf.tensor_scatter_nd_update(dy, [[targets[t], 0]], [dy[targets[t], 0] - 1]) # backprop into y
    dWhy += tf.matmul(dy, tf.transpose(hs[t]))
    dby += dy
    dh = tf.matmul(tf.transpose(Why), dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += tf.matmul(dhraw, tf.transpose(xs[t]))
    dWhh += tf.matmul(dhraw, tf.transpose(hs[t-1]))
    dhnext = tf.matmul(tf.transpose(Whh), dhraw)
  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
    tf.clip_by_value(dparam, -5, 5) # clip to mitigate exploding gradients
  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

"""function"""

def sample(h, seed_ix, n):
  """
  sample a sequence of integers from the model
  h is memory state, seed_ix is seed letter for first time step
  """
  x = tf.zeros([vocab_size, 1])
  x = tf.tensor_scatter_nd_update(x, [[seed_ix, 0]], [1])
  ixes = []
  for t in range(n):
    h = tf.tanh(tf.matmul(Wxh, x) + tf.matmul(Whh, h) + bh)
    y = tf.matmul(Why, h) + by
    p = tf.exp(y) / tf.reduce_sum(tf.exp(y))
    ix = tf.random.categorical(tf.transpose(p), 1)[0][0]
    x = tf.zeros([vocab_size, 1])
    x = tf.tensor_scatter_nd_update(x, [[ix.numpy(), 0]], [1])
    ixes.append(ix.numpy())
  return ixes

"""more"""

n, p = 0, 0
mWxh, mWhh, mWhy = tf.zeros_like(Wxh), tf.zeros_like(Whh), tf.zeros_like(Why)
mbh, mby = tf.zeros_like(bh), tf.zeros_like(by) # memory variables for Adagrad
smooth_loss = -tf.math.log(tf.constant(1.0/vocab_size)*seq_length) # loss at iteration 0


"""# RUN"""

while True:
  # prepare inputs (we're sweeping from left to right in steps seq_length long)
  if p+seq_length+1 >= len(data) or n == 0:
    hprev = tf.zeros([hidden_size,1]) # reset RNN memory
    p = 0 # go from start of data
  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

  # sample from the model now and then
  if n % 100 == 0:
    sample_ix = sample(hprev, inputs[0], 200)
    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
    print ('----\n %s \n----' % (txt, ))

  # forward seq_length characters through the net and fetch gradient
  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
  smooth_loss = smooth_loss * 0.999 + loss * 0.001
  if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress

  # perform parameter update with Adagrad
  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],
                                [dWxh, dWhh, dWhy, dbh, dby],
                                [mWxh, mWhh, mWhy, mbh, mby]):
    mem.assign_add(dparam * dparam)
    param.assign_add(-learning_rate * dparam / tf.sqrt(mem + 1e-8)) # adagrad update

  p += seq_length # move data pointer
  n += 1 # iteration counter
